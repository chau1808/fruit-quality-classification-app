# -*- coding: utf-8 -*-
"""vgg16_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AeWmM730rlAAZ6YjmrDX6nOZE4eObElQ
"""

# %% [colab-cell]
# === VGG16 fine-tune optimized for Colab + Google Drive dataset ===

import os, json, random, itertools, shutil
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
import torch.backends.cudnn as cudnn

from torchvision import datasets, models, transforms
from torchvision.models import VGG16_Weights
from torch.utils.data import DataLoader
from sklearn.metrics import classification_report, confusion_matrix

# ----------------- Config -----------------
from google.colab import drive
drive.mount('/content/drive')

# ƒê∆Ø·ªúNG D·∫™N G·ªêC D·ªÆ LI·ªÜU TR√äN DRIVE (ƒë√£ chu·∫©n ImageFolder 2 l·ªõp)
DRIVE_DATA_DIR = "/content/drive/MyDrive/processed_fruit_dataset"  # <-- s·ª≠a n·∫øu kh√°c
LOCAL_DATA_DIR = "/content/dataset"       # b·∫£n sao c·ª•c b·ªô ƒë·ªÉ train nhanh
COPY_LOCAL     = True                     # copy t·ª´ Drive v·ªÅ local (r·∫•t n√™n b·∫≠t)
OVERWRITE_COPY = True                     # x√≥a local c≈© tr∆∞·ªõc khi copy

# N∆°i l∆∞u output (tr√™n Drive)
SAVE_DIR   = "/content/drive/MyDrive"
MODEL_PATH = os.path.join(SAVE_DIR, "vgg16_fruit_model_2cls.pth")
CLASSES_JSON = os.path.join(SAVE_DIR, "classes.json")
METRICS_JSON = os.path.join(SAVE_DIR, "metrics.json")
CM_PNG       = os.path.join(SAVE_DIR, "confusion_matrix.png")
CM_CSV       = os.path.join(SAVE_DIR, "confusion_matrix.csv")

# Train params
BATCH_SIZE   = 64               # tƒÉng n·∫øu VRAM cho ph√©p (32/64/96)
NUM_EPOCHS   = 10
LR           = 1e-4
WEIGHT_DECAY = 1e-4
PATIENCE     = 3
SEED         = 42

# Dataloader params (t·ªëi ∆∞u I/O)
CPU_COUNT = os.cpu_count() or 2
NUM_WORKERS = min(8, max(2, CPU_COUNT - 1))   # v√≠ d·ª•: 3/7 workers t√πy m√°y
PREFETCH_FACTOR = 4
PIN_MEMORY = True

# ----------------- Seed & device -----------------
def set_seed(seed=42):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)

set_seed(SEED)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
cudnn.benchmark = True
try:
    torch.set_float32_matmul_precision('high')
except Exception:
    pass
print("üñ•Ô∏è Device:", device, "| CPU:", CPU_COUNT, "| workers:", NUM_WORKERS)

# ----------------- Copy dataset to local (fast I/O) -----------------
if COPY_LOCAL:
    if OVERWRITE_COPY and os.path.exists(LOCAL_DATA_DIR):
        shutil.rmtree(LOCAL_DATA_DIR)
    if not os.path.exists(LOCAL_DATA_DIR):
        print("üì¶ Copying dataset from Drive ‚Üí local ...")
        shutil.copytree(DRIVE_DATA_DIR, LOCAL_DATA_DIR)
        print("‚úÖ Copied to:", LOCAL_DATA_DIR)
    DATA_DIR = LOCAL_DATA_DIR
else:
    DATA_DIR = DRIVE_DATA_DIR
print("üìÅ Using DATA_DIR:", DATA_DIR)

# ----------------- Transforms -----------------
imagenet_mean = [0.485, 0.456, 0.406]
imagenet_std  = [0.229, 0.224, 0.225]

train_tfms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),
    transforms.ToTensor(),
    transforms.Normalize(imagenet_mean, imagenet_std),
])
eval_tfms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(imagenet_mean, imagenet_std),
])

# ----------------- Datasets & Loaders -----------------
train_ds = datasets.ImageFolder(os.path.join(DATA_DIR, "train"), transform=train_tfms)
val_ds   = datasets.ImageFolder(os.path.join(DATA_DIR, "val"),   transform=eval_tfms)
test_ds  = datasets.ImageFolder(os.path.join(DATA_DIR, "test"),  transform=eval_tfms)

class_names = train_ds.classes
num_classes = len(class_names)
assert num_classes >= 2, f"Dataset c·∫ßn ‚â•2 l·ªõp. Hi·ªán c√≥: {class_names}"
print(f"üìö Classes ({num_classes}): {class_names}")

train_loader = DataLoader(
    train_ds, batch_size=BATCH_SIZE, shuffle=True,
    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,
    persistent_workers=True, prefetch_factor=PREFETCH_FACTOR
)
val_loader = DataLoader(
    val_ds, batch_size=BATCH_SIZE, shuffle=False,
    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,
    persistent_workers=True, prefetch_factor=PREFETCH_FACTOR
)
test_loader = DataLoader(
    test_ds, batch_size=BATCH_SIZE, shuffle=False,
    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,
    persistent_workers=True, prefetch_factor=PREFETCH_FACTOR
)

# ----------------- Model -----------------
model = models.vgg16(weights=VGG16_Weights.DEFAULT)
for p in model.features.parameters():
    p.requires_grad = False
model.classifier[6] = nn.Linear(4096, num_classes)

# channels_last ƒë·ªÉ t·ªëi ∆∞u Conv
model = model.to(memory_format=torch.channels_last).to(device)

# PyTorch 2 compile (tƒÉng t·ªëc)
try:
    model = torch.compile(model)
    print("‚ö° torch.compile enabled")
except Exception as e:
    print("‚ÑπÔ∏è torch.compile not available:", e)

criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WEIGHT_DECAY)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="max", factor=0.5, patience=1, verbose=True)
scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())

# ----------------- Train/Eval -----------------
def train_one_epoch():
    model.train()
    loss_sum, correct, total = 0.0, 0, 0
    for x, y in train_loader:
        # d√πng channels_last cho tensor input
        x = x.to(device, non_blocking=True).to(memory_format=torch.channels_last)
        y = y.to(device, non_blocking=True)

        optimizer.zero_grad(set_to_none=True)
        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
            logits = model(x)
            loss = criterion(logits, y)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        pred = logits.argmax(1)
        loss_sum += loss.item() * x.size(0)
        correct  += (pred == y).sum().item()
        total    += y.size(0)
    return loss_sum/max(total,1), correct/max(total,1)

@torch.no_grad()
def evaluate(loader):
    model.eval()
    loss_sum, correct, total = 0.0, 0, 0
    for x, y in loader:
        x = x.to(device, non_blocking=True).to(memory_format=torch.channels_last)
        y = y.to(device, non_blocking=True)
        logits = model(x)
        loss = criterion(logits, y)
        loss_sum += loss.item() * x.size(0)
        pred = logits.argmax(1)
        correct += (pred == y).sum().item()
        total += y.size(0)
    return loss_sum/max(total,1), correct/max(total,1)

best_val_acc, best_state, patience = 0.0, None, 0
history = []

for epoch in range(1, NUM_EPOCHS + 1):
    tr_loss, tr_acc = train_one_epoch()
    va_loss, va_acc = evaluate(val_loader)
    scheduler.step(va_acc)

    history.append({
        "epoch": epoch,
        "train_loss": float(tr_loss), "train_acc": float(tr_acc),
        "val_loss": float(va_loss),   "val_acc":  float(va_acc),
    })
    print(f"Epoch {epoch:02d}/{NUM_EPOCHS} | "
          f"Train: loss {tr_loss:.4f} acc {tr_acc:.4f} | "
          f"Val:   loss {va_loss:.4f} acc {va_acc:.4f}")

    if va_acc >= best_val_acc:
        best_val_acc, best_state, patience = va_acc, model.state_dict(), 0
        print("üíæ Best checkpoint updated.")
    else:
        patience += 1
        if patience >= PATIENCE:
            print("‚èπÔ∏è Early stopping.")
            break

# Load best
if best_state is not None:
    model.load_state_dict(best_state)
print("üèÅ Best val acc:", best_val_acc)

# ----------------- Test & Reports -----------------
@torch.no_grad()
def predict_all(loader):
    model.eval()
    ys, ps = [], []
    for x, y in loader:
        x = x.to(device, non_blocking=True).to(memory_format=torch.channels_last)
        logits = model(x)
        pred = logits.argmax(1).cpu().numpy()
        ys.extend(y.numpy()); ps.extend(pred)
    return np.array(ys), np.array(ps)

te_loss, te_acc = evaluate(test_loader)
y_true, y_pred = predict_all(test_loader)

print(f"\nüß™ Test ‚Üí loss: {te_loss:.4f} | acc: {te_acc:.4f}\n")
print(classification_report(y_true, y_pred, target_names=class_names, digits=4))

cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))

def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix'):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)
    fig, ax = plt.subplots(figsize=(5 + 0.6*len(classes), 4 + 0.6*len(classes)))
    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    ax.figure.colorbar(im, ax=ax)
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           xticklabels=classes, yticklabels=classes,
           ylabel='True label', xlabel='Predicted label',
           title=title)
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        ax.text(j, i, format(cm[i, j], fmt),
                ha="center", va="center",
                color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return fig

fig = plot_confusion_matrix(cm, class_names, normalize=False, title='Confusion Matrix')
plt.savefig(CM_PNG, dpi=150, bbox_inches='tight')
plt.show()
print("üìÑ Saved confusion matrix PNG:", CM_PNG)

pd.DataFrame(cm, index=class_names, columns=class_names).to_csv(CM_CSV, encoding="utf-8-sig")
print("üìÑ Saved confusion matrix CSV:", CM_CSV)

# ----------------- Save artifacts -----------------
os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
torch.save(model.state_dict(), MODEL_PATH)
with open(CLASSES_JSON, "w", encoding="utf-8") as f:
    json.dump({"classes": class_names}, f, ensure_ascii=False, indent=2)
with open(METRICS_JSON, "w", encoding="utf-8") as f:
    json.dump({
        "best_val_acc": float(best_val_acc),
        "test_loss": float(te_loss),
        "test_acc": float(te_acc),
        "history": history
    }, f, ensure_ascii=False, indent=2)

print(f"‚úÖ Model saved: {MODEL_PATH}")
print(f"‚úÖ classes.json saved: {CLASSES_JSON}")
print(f"‚úÖ metrics.json saved: {METRICS_JSON}")